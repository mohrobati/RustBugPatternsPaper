Automated program repair aims to debug faulty source code without human involvement, sometimes using test cases to guide the repair. Before a repair tool modifies code, it will typically use a fault localization module to find fault locations. A fault localization module ranks program statements based on suspiciousness; one example is Tarantula by \cite{jones2005empirical}. Suspiciousness is calculated using a statistical model which relies on the observation that buggy statements are executed mostly by failed test cases~\citep{naish2009spectral,xie2013theoretical}.

Researchers have proposed different methods for code repair, each of which can be said to follow a different mindset. Following the taxonomy proposed by \cite{liu2018survey}, we briefly present two mindsets, and will discuss how having a set of predefined fix patterns can aid repair.

\subsection{Search-Based Program Repair}
One line of research in program repair follows the competent programmer hypothesis~\citep{gopinath2014mutant}: syntactically, a faulty program is not that far away from its correct version. This hypothesis suggests the search-based program repair mindset. If the hypothesis holds, we can develop mutation operators (which change an expression or a statement) and apply them to a list of fault locations provided by the fault localization module. So, we loop through possible mutations; if the mutation of a statement does not cause all expected-successful test cases to succeed, we assume that the statement is correct and move on to the next fault location. Having a set of bug-producing patterns, like the ones we are proposing, will help researchers design targetted mutation operators that leverage domain specific insights.

Use of mutation operators in program repair became popular when \cite{forrest2009genetic} and \cite{nguyen2009using} developed GenProg, a program repair tool that uses genetic programming. The main idea behind this tool is that statements follow certain patterns in a codebase. Therefore, if we find the correct version of a faulty statement somewhere in the program, we can use that version in place of the faulty original. This tool showed promising results as it managed to find patches without any additional annotations or human involvements.

However, \cite{arcuri2011practical} observed that GenProg mostly found the patch while carrying out random initialization---that is, before GenProg's evolution begins. Their tools TrpAutoRepair~\citep{qi2013efficient} and RSRepair~\citep{qi2014strength} advocate that test case prioritization is necessary while using genetic programming, as fitness evaluation is an expensive part of the repair pipeline.

\cite{tan2015relifix} used mutation repairing for fixing regression bugs. They categorized common code changes in real-world regressions after studying 73 program evolution benchmarks; our approach also yields a set of common code changes, for Rust, using a different set of changes. Tan and Rovehoudhury used their categorization to design mutation operators, and drew from those operators to repair faulty program locations.

\subsection{Pattern-based Program Repair and Mining Bug Fix Patterns}

The main intuition in pattern-based program repair methods is that bug fixes follow certain patterns. Thus, having a collection of common bug fix patterns is a crucial step for developing such repair tools. Studying bug fix patterns can be traced back to 1975 when Enders et al introduced a classification of bugs that occur in DOS/VS operating system. This work contends that categorizing bug patterns can aid in uncovering error causes, fixing them, and preventing their recurrence. Flanagan et al. proposed MrSpidey, an interactive static debugger for Scheme (a dialect of Lisp). MrSpidey allows the programmer to browse program invariants and their derivations over error prone patterns (unsafe operators). The user can determine whether the error will actually occur, or if obtaining the correctness proof is unattainable due to the tool's limitations.

More recently, \cite{pan2009toward} analyzed seven large-scale widely used Java projects and obtained 27 common bug fix patterns. They used a bug fix pattern extractor tool to automatically parse and detect bug patterns within the bug fix hunks with less than seven statements. They ignored larger bug fix hunks as they tended to exhibit random changes and did not have observable meaningful patterns. Though they did the analysis on Java projects, their reported patterns were not specific to Java. \cite{martinez2015mining,martinez2012mining} exploited the frequency of observed patterns to introduce a heuristic patch searching method. In their empirical evaluation, they concluded that choosing repair actions probabilistically (weighted by frequency) can help with reducing the search space, hence creating a more effective repair tool.

\cite{hanam2016discovering} conducted similar research, but for finding pervasive bug fix patterns in JavaScript. They performed a large-scale study of bug fix patterns by mining 105K commits from 134 server-side JavaScript projects. Like us, they used the DBSCAN clustering algorithm and divided bug fixing change types into 219 clusters, from which they extracted 13 pervasive cross-project bug fix patterns. 

\cite{yang2022mining} proposed a mining approach to detect Python bug fix patterns by studying fine-grained fixing code changes. They also examined how many bugs could be fixed using automated bug fixing approaches. Moreover, they evaluated the fix patterns that they detected and concluded that 37\% of the buggy codes could be matched by the fix patterns they had found. 

We would characterize our work as the Rust version of what was done in~\cite{hanam2016discovering} and~\cite{yang2022mining}. Like~\cite{hanam2016discovering}, we used DBSCAN to cluster fixes. Moreover, similar to~\cite{yang2022mining}, we introduced patterns in two different categories. In our case there were general and language-specific patterns; for Rust, the language-specific patterns were related to the borrow checker (BC patterns). However, unlike our paper, \cite{yang2022mining} needed to do a much more elaborate manual analysis, as they had only collected general information about the single hunk changes (e.g. the number of variables, or arguments).

\subsection{Automated Patch Correctness Assessment}

Overfitting poses a substantial challenge for automated program repair (APR) systems, where synthesized patches may pass test cases but fail to be correct. Automated patch correctness assessment (APCA) aims to inspect the correctness of generated patches using dynamic and static analyses. While our project has a different objective than APCA, we share similarities in design decisions with APCA systems, particularly in feature extraction. \cite{ye2021automated} propose ODS, an overfitting patch detection system, which assumes that code features capturing universal correctness properties can be utilized to classify overfitting patches across program repair systems and software projects. ODS extracts 202 manually crafted code features by comparing fixed and buggy code, and employs supervised learning to build a distributional model for classifying unseen patches, achieving over 70\% accuracy in evaluations on Defects4J, Bugs.jar, and Bears benchmarks. Similarly, our feature extraction involves manual crafting of features. In another study, \cite{tian2022change} present an enhanced APCA model that transforms the problem into a question-answering scenario and incorporates a neural architecture to learn semantic correlations between bug reports and commit messages. Additionally, \cite{lin2022context} introduce \textsc{Cache}, a neural-based context-aware AST embedding method that captures both the changed part and the unchanged part (context code) of patches. Evaluation results demonstrate that context knowledge significantly enhances the performance and accuracy of systems that attempt to automatically assess patch correctness (which is not our goal).


\subsection{Code Embedding}

Recent advances in deep learning have motivated researchers to build models for various types of data. Machine learning models require a numerical representation of the data to feed into the model. To realize this mapping for natural language text, researchers have proposed \emph{word embedding} to embed word information in fixed size vectors. As programming languages are not that different from natural languages~\citep{hindle2016naturalness}, similar mappings have been suggested for computer programs~\citep{chen2019literature}.

\cite{alon2019code2vec} presented code2vec, a neural model for representing code snippets as fixed size vectors. In their approach, they extracted flattened paths from the AST, and stored all the leaf-to-leaf paths. The intuition behind this decision was that leaf-to-leaf paths tend to encode more semantic information than root-to-leaf paths. They associated a fixed size vector with each path, and fed these as inputs to a neural network that learns how to aggregate all these paths to a single embedding. Our code embedding approach is simpler than code2vec's, as we have a specific (and simpler) goal---we aim to cluster bug patterns, not predict method names. Moreover, unlike us, code2vec does not offer a general code embedding method, since the final embedding depends on the output layer of the neural network. That is, if we change the goal of method name prediction, the embeddings would be completely different. Also, unlike many embeddings, our code embedding is interpretable: our columns are associated with non terminals. We have used the interpretability in our experiments. In code2vec, the layout of the final vector is a parameter of the system and  is not related to the underlying programming language. Building on the same ideas, as a subsequent work, the same authors also introduced code2seq~\citep{alon2019code2seq}, an approach for transforming a code snippet to a sequence encoding. They evaluated their work on three seq2seq use cases: (1) method name prediction, (2) code captioning, and (3) code documentation. code2seq proved to have a better understanding of syntactical structure of the code and outperformed previous neural machine translation systems.

\cite{hoang2020cc2vec} introduced CC2Vec, a neural network model that utilizes log messages accompanying code changes to learn representations capturing semantic information. By addressing challenges in distinguishing between added and removed code and incorporating an attention mechanism, CC2Vec outperforms previous techniques for code change representation. The model employs a hierarchical attention network, which incorporates a multi-level bidirectional GRU recurrent neural network, to encode information about changed code tokens, lines, and hunks into vectors representing added and removed changes. These vectors are then combined and fed into a hidden layer for predicting the target function. Evaluation across log message generation, bug fixing patch identification, and just-in-time defect prediction tasks demonstrates the superior performance of CC2Vec compared to state-of-the-art approaches in all three tasks.

Our work tends to be more similar to that of~\cite{hanam2016discovering}. However, our novel program embedding approach differs from what they did on certain key decisions. In~\cite{hanam2016discovering}, they introduced the Feature Properties table, which is a categorization for program elements. Embedding programs based on this table resulted in an order-sensitive embedding. However, it made the datapoints sparse, and our datapoints are more dense than theirs. More specifically, the number of our dimensions is more than 11$\times$ smaller.

\subsection{Refactoring tools for Rust}
Clippy is widely known as the best code refactoring tool for Rust. It can identify over 600 common mistakes and offer automated fixes. It provides users with the flexibility to adjust the linting level, allowing them to control the number of lint suggestions they receive. As discussed in Section~\ref{sec:bc_patterns}, we found that Clippy can detect and address five of our borrow-checker-related patterns. \cite{sam2017automated} introduced a refactoring tool that uses the Rust compiler infrastructure to perform various refactoring operations, including renaming variables, arguments, fields, functions, methods, structs, and enumerations, as well as inlining local variables, and reifying and eliding lifetime parameters. In a more advanced use case, \cite{ling2022rust} proposed CRustS, a tool designed for refactoring C2Rust output. C2Rust is a C to Rust transpiler that converts C code into Rust syntax but retains the unsafe semantics of C. By relaxing the constraint of preserving semantics during transformations, CRustS increases the proportion of transformed code that successfully passes the safety checks of the Rust compiler.

\subsection{Code Patterns in Rust}

To our knowledge, our work is the first to use an automated mining and code analysis pipeline to find pervasive patterns in Rust open source projects, even if previous studies have (manually) investigated common bug patterns in Rust.

\cite{qin2020understanding} conducted the first empirical study on real-world Rust program behaviours. They manually inspected 850 unsafe code usages and 17 bugs across five open-source Rust projects, five Rust libraries, two online security databases, and the Rust standard library. They analyzed the motivation behind unsafe code usage and removal, in addition to recording 70 memory-safety issues and 100 concurrency bugs. They also provided Rust programmers with some suggestions and insights to develop better Rust programs. Using the results of their manual analysis, they designed two bug detectors, and provided recommendations for developing more bug detectors in the future. Our automated approach to finding bug patterns has similar implications to their work but can operate at a far larger scale.

\cite{li2021mirchecker} present MirChecker, a fully automated bug detection framework for Rust. This framework works by carrying out static analysis on Rust's Mid-level Intermediate Representation (MIR). The tool exploits the insights obtained from manually observing existing bugs (by studying reported CVEs) in Rust code bases. Using both numerical (e.g. integer bounds) and symbolic (e.g. modelling memory) information, the framework detects errors by using constraint solving techniques. MirChecker detected 33 new bugs, including 16 memory safety faults, across 12 Rust crates.

