In this work, we used Ruxanne to successfully mine 20 cross-project bug fix patterns. We presented these patterns in two groups: 12 general patterns and 8 BC-related patterns. Clippy is able to detect five of the BC-related patterns. Fundamentally, Clippy cannot detect some of the remaining patterns. For instance, we proposed the pattern \textit{Adding mutability}. In the Rust context, this change anticipates a future change to the code which will perform a mutation of a previously immutable value. Predicting the future is beyond Clippy's remit. To the best of our knowledge, the rest of the patterns cannot be reported by the current linting tools. 

In each group, we categorized the bug fix patterns based on their underlying program element. The patterns encompass a wide variety of program elements both in the general and BC-related groups. This shows that our weighting scheme is not biased towards specific program elements. Also, our patterns are all cross-project fix patterns: each fix pattern has been seen at least in three different projects.

Furthermore, we expected to obtain some patterns associated with certain Rust-related features. However, our results did not include those patterns. For instance, we did not observe any patterns associated with the Rust \verb+move+ operator, which has been known to trouble many beginning Rust programmers. \verb+move+ is used to transfer the ownership of values to a closure. In some cases (e.g. multithreading), not moving the ownership could create invalid references. Rust compiler checks prevent such errors. Our speculation is that, since Rust enforces strict compiler checks, programmers usually get many errors, and they only push their code once it is free of compile errors. We thus would not observe patterns which don't make it to the repository.

We believe our results can help researchers to create IDE tools for Rust. For instance, we discussed that \textit{Modifying the attributes of structs} is a common pattern that is observed in bug fix code changes. Assuming that the correct usage of attributes can be found other locations in the codebase~\citep{forrest2009genetic}, a code linter tool can be crafted to look for correct attribute lists.

Similarly, we presented the pattern \textit{Dropping clone and adding borrowing}. As discussed, Clippy does not detect this pattern, and repetitive clones can be computationally expensive and might lead to hogging the CPU. Using state-of-the-art methods for program repair, one could design a tool to recognize this pattern and change variable cloning to simple borrowing.

A general approach for code embedding is to extract paths with semantic information that better serve a given embedding goal. For instance, to design an embedding method for repairing Rust programs, the designer might want to disregard certain elements, such as moving ownership to closure bodies. Such elements may be irrelevant for repair because they did not appear in the bug patterns that we presented.

All of our parsing, path extraction, weighting scheme, and clustering modules, along with our final results can be found in our replication package, which we provide for verification, reusability and further extension. Weightings can be modified to make the embedding focus on a specific set of elements for building different code embeddings.

\subsection{Threats to Validity}

In our work there are two main threats to internal validity: (1) A threat to internal validity is confounding, where changes to what shows up in the embedding are not due to changes in the code being embedded. In this case, the weighting scheme may contribute to confounding because we manually adjusted the weights. Nevertheless, our weighting scheme can be readjusted to find different patterns, which is why we made our pipeline publicly available. (2) Our code embedding approach is based on the frequency of observed program elements in ASTs, and we use DBSCAN as our clustering algorithm. Other code embedding methods and clustering algorithms (e.g. SLINK) might output new clusters that our pipeline is unable to find. A threat to external validity is selection bias, where the benchmarks differ systematically from the actual world. The selected projects might cause selection bias and therefore we might not have presented patterns that may exist in other projects. Another threat to external validity can be the developers reporting the commits as bug fixing commits while they are not really fixing a functionality. Similarly, a commit message might not contain our target keywords while the commit is associated with bug fixing changes.
